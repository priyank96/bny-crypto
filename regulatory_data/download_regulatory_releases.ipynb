{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "def federal_reserve_datetime_conversion(date_string, time_string):\n",
    "    \"\"\"\n",
    "    Convert date and time string from Federal Reserve press release format to UTC datetime object\n",
    "    eg.. date_string = 'January 06, 2021'\n",
    "         time_string = '2:00 p.m. EST'\n",
    "    \"\"\"\n",
    "\n",
    "    time_string = time_string.split(' ')\n",
    "    if time_string[1] == 'p.m.' and time_string[0].split(':')[0] != '12':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 12) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[1] == 'a.m.' and time_string[0].split(':')[0] == '12':\n",
    "        time_string[0] = '00:' + time_string[0].split(':')[1]\n",
    "    if time_string[2] == 'EST':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 5) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[2] == 'EDT':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 4) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[2] == 'UTC':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Timezone not recognized: {time_string}')\n",
    "\n",
    "    time_string = time_string[0]\n",
    "\n",
    "    # Parse the time string into a time object\n",
    "    time_obj = datetime.datetime.strptime(time_string, '%H:%M')\n",
    "    # Get the current date in Eastern time zone\n",
    "    date_obj = datetime.datetime.strptime(date_string, '%B %d, %Y').date()\n",
    "\n",
    "    # Combine the date and time objects\n",
    "    date_obj = datetime.datetime.combine(date_obj, time_obj.time())\n",
    "\n",
    "    return str(date_obj)\n",
    "\n",
    "federal_reserve_datetime_conversion('January 06, 2021', '3:45 p.m. EDT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df_columns = ['timestamp', 'url', 'title', 'text', 'summary', 'source', 'tags', 'type', 'language', 'country']\n",
    "\n",
    "parse_urls = [\n",
    "    {   'url': 'https://www.federalreserve.gov/newsevents/pressreleases/',\n",
    "        'source': 'Federal Reserve',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "        ]\n",
    "\n",
    "url_parsed_dict = dict()\n",
    "regulatory_pr_df = pd.DataFrame()\n",
    "debugging = True\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "for parse_url in parse_urls:\n",
    "\n",
    "    if parse_url['source'] == 'Federal Reserve':\n",
    "        \n",
    "        for n in range(int((end_date - start_date).days)):\n",
    "            for category in ['monetary', 'other', 'orders', 'bcreg', 'enforcement']:\n",
    "\n",
    "                char_append = 'a'\n",
    "                while True:\n",
    "                    response = requests.get(parse_url['url'] + category + (start_date + datetime.timedelta(n)).strftime('%Y%m%d') + char_append + '.htm')\n",
    "                    # print(reponse.url)\n",
    "                    if response.status_code != 200:\n",
    "                        break\n",
    "\n",
    "                    # increment char_append\n",
    "                    char_append = chr(ord(char_append) + 1)\n",
    "\n",
    "                    if debugging:\n",
    "                        print(f\"Valid: {response.url}\")\n",
    "                    processed_row = dict() \n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    date_string = soup.find('p', class_='article__time').text.strip()\n",
    "                    time_string = soup.find('p', class_='releaseTime')\n",
    "                    if 'immediate' in time_string.text:\n",
    "                        time_string = '00:00 a.m. UTC'\n",
    "                    else:\n",
    "                        time_string = time_string.text.split('at')[-1].split('T')[0].strip() + 'T'\n",
    "\n",
    "                    time_string = time_string.replace('  ', ' ')\n",
    "\n",
    "                    processed_row['timestamp'] = federal_reserve_datetime_conversion(date_string, time_string)\n",
    "                    if debugging:\n",
    "                        print(processed_row['timestamp'], date_string, time_string)\n",
    "\n",
    "                    processed_row['url'] = response.url\n",
    "                    if processed_row['url'] not in url_parsed_dict:\n",
    "                        url_parsed_dict[processed_row['url']] = True\n",
    "                    else:\n",
    "                        continue\n",
    "                    processed_row['title'] = soup.find('h3', class_='title').text.strip()\n",
    "                    processed_row['text'] = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8').text.strip()\n",
    "                    processed_row['summary'] = ''\n",
    "                    processed_row['source'] = parse_url['source']\n",
    "                    processed_row['tags'] = parse_url['tags']\n",
    "                    processed_row['type'] = parse_url['type']\n",
    "                    processed_row['language'] = parse_url['language']\n",
    "                    processed_row['country'] = parse_url['country']\n",
    "\n",
    "                    if debugging:\n",
    "                        print(processed_row)\n",
    "\n",
    "                    processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "                    regulatory_pr_df = pd.concat([regulatory_pr_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "regulatory_pr_df.to_csv('regulatory_pr_fed.csv', index=False)\n",
    "regulatory_pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regulatory_pr.csv')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_urls = [\n",
    "    {   'url': 'https://www.federalreserve.gov/newsevents/pressreleases/',\n",
    "        'source': 'Federal Reserve',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.fdic.gov/news/press-releases/2021/pr21\",\n",
    "        'source': 'FDIC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.fdic.gov/news/press-releases/2022/pr22\",\n",
    "        'source': 'FDIC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=crypto&year=2021&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'crypto',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=&year=2021&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=crypto&year=2022&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'crypto',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=&year=2022&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "        ]\n",
    "\n",
    "url_parsed_dict = dict()\n",
    "regulatory_pr_df = pd.DataFrame()\n",
    "debugging = False\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "for parse_url in parse_urls:\n",
    "\n",
    "    if parse_url['source'] == 'Federal Reserve':\n",
    "        \n",
    "        for n in range(int((end_date - start_date).days)):\n",
    "            for category in ['monetary', 'other', 'orders', 'bcreg', 'enforcement']:\n",
    "\n",
    "                char_append = 'a'\n",
    "                while True:\n",
    "                    response = requests.get(parse_url['url'] + category + (start_date + datetime.timedelta(n)).strftime('%Y%m%d') + char_append + '.htm')\n",
    "                    # print(reponse.url)\n",
    "                    if response.status_code != 200:\n",
    "                        break\n",
    "\n",
    "                    # increment char_append\n",
    "                    char_append = chr(ord(char_append) + 1)\n",
    "\n",
    "                    if debugging:\n",
    "                        print(f\"Valid: {response.url}\")\n",
    "                    processed_row = dict() \n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    date_string = soup.find('p', class_='article__time').text.strip()\n",
    "                    time_string = soup.find('p', class_='releaseTime')\n",
    "                    if 'immediate' in time_string.text:\n",
    "                        time_string = '00:00 a.m. UTC'\n",
    "                    else:\n",
    "                        time_string = time_string.text.split('at')[-1].split('T')[0].strip() + 'T'\n",
    "\n",
    "                    time_string = time_string.replace('  ', ' ')\n",
    "\n",
    "                    processed_row['timestamp'] = federal_reserve_datetime_conversion(date_string, time_string)\n",
    "                    if debugging:\n",
    "                        print(processed_row['timestamp'], date_string, time_string)\n",
    "\n",
    "                    processed_row['url'] = response.url\n",
    "                    if processed_row['url'] not in url_parsed_dict:\n",
    "                        url_parsed_dict[processed_row['url']] = True\n",
    "                    else:\n",
    "                        continue\n",
    "                    processed_row['title'] = soup.find('h3', class_='title').text.strip()\n",
    "                    processed_row['text'] = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8').text.strip()\n",
    "                    processed_row['summary'] = ''\n",
    "                    processed_row['source'] = parse_url['source']\n",
    "                    processed_row['tags'] = parse_url['tags']\n",
    "                    processed_row['type'] = parse_url['type']\n",
    "                    processed_row['language'] = parse_url['language']\n",
    "                    processed_row['country'] = parse_url['country']\n",
    "\n",
    "                    if debugging:\n",
    "                        print(processed_row)\n",
    "\n",
    "                    processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "                    regulatory_pr_df = pd.concat([regulatory_pr_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "    if parse_url['source'] == 'FDIC':\n",
    "\n",
    "        i = 1\n",
    "        while True:\n",
    "            response = requests.get(parse_url['url'] + str(i).zfill(3) + '.html')\n",
    "            if response.status_code != 200:\n",
    "                if debugging:\n",
    "                    print(f'pr21{str(i).zfill(3)}.html not found. Breaking loop.')\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "            processed_row = dict() \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            date_string = soup.find('span', class_='prdate').text.strip()\n",
    "            if len(date_string.split(',')) == 3:\n",
    "                processed_row['timestamp'] = datetime.datetime.strptime(date_string, '%A, %B %d, %Y').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            elif len(date_string.split(',')) == 2:\n",
    "                processed_row['timestamp'] = datetime.datetime.strptime(date_string, '%B %d, %Y').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            processed_row['url'] = response.url\n",
    "            if processed_row['url'] not in url_parsed_dict:\n",
    "                url_parsed_dict[processed_row['url']] = True\n",
    "            else:\n",
    "                continue\n",
    "            processed_row['title'] = soup.find_all('div', class_='prtitle')[0].find('h1').text.strip()\n",
    "            \n",
    "            processed_row['text'] = ''\n",
    "            for paragraph in soup.find('article', class_='order-2').find_all('p'):\n",
    "                processed_row['text'] += paragraph.text.strip() + '\\n'\n",
    "            processed_row['summary'] = ''\n",
    "            processed_row['source'] = parse_url['source']\n",
    "            processed_row['tags'] = parse_url['tags']\n",
    "            processed_row['type'] = parse_url['type']\n",
    "            processed_row['language'] = parse_url['language']\n",
    "            processed_row['country'] = parse_url['country']\n",
    "\n",
    "            if debugging:\n",
    "                print(processed_row)\n",
    "\n",
    "            processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "            regulatory_pr_df = pd.concat([regulatory_pr_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "    if parse_url['source'] == 'SEC':\n",
    "\n",
    "        # continue\n",
    "\n",
    "        response = requests.get(parse_url['url'])\n",
    "        # print(response.status_code)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        articles = soup.find_all('tr', class_='pr-list-page-row')\n",
    "\n",
    "        for article in reversed(articles):\n",
    "\n",
    "\n",
    "            processed_row = dict() \n",
    "            row = article.find_all('td')\n",
    "\n",
    "            # 1. Time\n",
    "            processed_row['timestamp'] = parser.parse(row[1].find('time')['datetime']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # 2. URL\n",
    "            processed_row['url'] = \"https://www.sec.gov\" + row[2].find('a')['href']\n",
    "            if processed_row['url'] not in url_parsed_dict:\n",
    "                url_parsed_dict[processed_row['url']] = True\n",
    "            else:\n",
    "                continue\n",
    "            # 3. Title\n",
    "            processed_row['title'] = row[2].find('a').text.strip()\n",
    "            # 4. Text\n",
    "            article_soup = BeautifulSoup(requests.get(processed_row['url']).content, 'html.parser')\n",
    "            processed_row['text'] = article_soup.find('div', class_='article-body').text.strip()\n",
    "            # 5. Summary\n",
    "            processed_row['summary'] = ''\n",
    "            # 6. Source\n",
    "            processed_row['source'] = parse_url['source']\n",
    "            # 7. Tags\n",
    "            processed_row['tags'] = parse_url['tags']\n",
    "            # 8. Type\n",
    "            processed_row['type'] = parse_url['type']\n",
    "            # 9. Language\n",
    "            processed_row['language'] = parse_url['language']\n",
    "            # 10. Country\n",
    "            processed_row['country'] = parse_url['country']\n",
    "\n",
    "            if debugging:\n",
    "                print(processed_row)\n",
    "\n",
    "            processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "            regulatory_pr_df = pd.concat([regulatory_pr_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "regulatory_pr_df['timestamp'] = pd.to_datetime(regulatory_pr_df['timestamp'])\n",
    "regulatory_pr_df = regulatory_pr_df.sort_values(by=['timestamp'])\n",
    "regulatory_pr_df = regulatory_pr_df[(regulatory_pr_df['timestamp'] >= '2021-03-01 00:00:00') & (regulatory_pr_df['timestamp'] <= '2022-09-19 17:30:00')]\n",
    "regulatory_pr_df.reset_index(drop=True, inplace=True)\n",
    "regulatory_pr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulatory_pr_df.to_csv('regulatory_pr.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and use regulatory_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulatory_pr_df = pd.read_csv('regulatory_pr.csv')\n",
    "regulatory_pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_keywords = ['cryptoinvestor',\n",
    "                  'btc',\n",
    "                  'cryptoexchange',\n",
    "                  'ethereum',\n",
    "                  'cryptocurrency',\n",
    "                  'crypto',\n",
    "                  'hodl',\n",
    "                  'crypto currency',\n",
    "                  'dogecoin',\n",
    "                  'kraken',\n",
    "                  'token',\n",
    "                  'cryptocurrencymarket',\n",
    "                  'gemini',\n",
    "                  'coinbase',\n",
    "                  'bitcoin',\n",
    "                  'ico',\n",
    "                  'altcoin',\n",
    "                  'cryptocurrencies',\n",
    "                  'binance',\n",
    "                  'cryptonews',\n",
    "                  'web3',\n",
    "                  'cryptocurrencyexchange',\n",
    "                  'doge',\n",
    "                  'crypto mining',\n",
    "                  '$eth',\n",
    "                  'blockchain',\n",
    "                  '$btc',\n",
    "                  'coin',\n",
    "                  'doge coin']\n",
    "\n",
    "crypto_related = list()\n",
    "\n",
    "for index, row in regulatory_pr_df.iterrows():\n",
    "    found = False\n",
    "    for keyword in crypto_keywords:\n",
    "        if keyword in row['text'].lower().split():\n",
    "            found = True\n",
    "            # print(f\"Found {keyword} in {row['url']}\")\n",
    "            break\n",
    "    if found:\n",
    "        crypto_related.append(1)\n",
    "    else:\n",
    "        crypto_related.append(0)\n",
    "\n",
    "\n",
    "regulatory_pr_df['crypto_related'] = crypto_related\n",
    "crypto_regulatory_pr_df =  regulatory_pr_df[regulatory_pr_df['crypto_related'] == 1]\n",
    "crypto_regulatory_pr_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crypto_regulatory_pr_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "except: \n",
    "    pass\n",
    "print(f'{device} is available')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.to(device)\n",
    "\n",
    "text = \"SVB bank has collapsed due to high interest rates\"\n",
    "def sentiment_analysis(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
    "    return predictions\n",
    "\n",
    "# predictions = sentiment_analysis(text)\n",
    "# print(f\"Negative: {predictions[0]:.2f}\")\n",
    "# print(f\"Neutral: {predictions[1]:.2f}\")\n",
    "# print(f\"Positive: {predictions[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentiment_list = list()\n",
    "negative_sentiment_list = list()\n",
    "for index, row in regulatory_pr_df.iterrows():\n",
    "    prompt = f\"{row['title']}\\n{row['text']}\"\n",
    "    predictions = sentiment_analysis(prompt[:512])\n",
    "\n",
    "    # print(f\"{prompt}\")\n",
    "    # print(f\"Negative: {predictions[0]:.2f}\")\n",
    "    # print(f\"Neutral: {predictions[1]:.2f}\")\n",
    "    # print(f\"Positive: {predictions[2]:.2f}\")\n",
    "    if predictions[0] > predictions[1] and predictions[0] > predictions[2]:\n",
    "        positive_sentiment_list.append(0)\n",
    "        negative_sentiment_list.append(1)\n",
    "    elif predictions[2] > predictions[0] and predictions[2] > predictions[1]:\n",
    "        positive_sentiment_list.append(1)\n",
    "        negative_sentiment_list.append(0)\n",
    "    else:\n",
    "        positive_sentiment_list.append(0)\n",
    "        negative_sentiment_list.append(0)\n",
    "        \n",
    "    \n",
    "regulatory_pr_df['positive_sentiment'] = positive_sentiment_list\n",
    "regulatory_pr_df['negative_sentiment'] = negative_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentiment_list = list()\n",
    "negative_sentiment_list = list()\n",
    "for index, row in crypto_regulatory_pr_df.iterrows():\n",
    "    prompt = f\"{row['title']}\\n{row['text']}\"\n",
    "    predictions = sentiment_analysis(prompt[:512])\n",
    "\n",
    "    # print(f\"{prompt}\")\n",
    "    # print(f\"Negative: {predictions[0]:.2f}\")\n",
    "    # print(f\"Neutral: {predictions[1]:.2f}\")\n",
    "    # print(f\"Positive: {predictions[2]:.2f}\")\n",
    "    if predictions[0] > predictions[1] and predictions[0] > predictions[2]:\n",
    "        positive_sentiment_list.append(0)\n",
    "        negative_sentiment_list.append(1)\n",
    "    elif predictions[2] > predictions[0] and predictions[2] > predictions[1]:\n",
    "        positive_sentiment_list.append(1)\n",
    "        negative_sentiment_list.append(0)\n",
    "    else:\n",
    "        positive_sentiment_list.append(0)\n",
    "        negative_sentiment_list.append(0)\n",
    "        \n",
    "    \n",
    "crypto_regulatory_pr_df['positive_sentiment'] = positive_sentiment_list\n",
    "crypto_regulatory_pr_df['negative_sentiment'] = negative_sentiment_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sentiment crypto news vs fmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "regulatory_pr_df = pd.read_csv('regulatory_pr.csv')\n",
    "crypto_regulatory_pr_df =  regulatory_pr_df[regulatory_pr_df['crypto_related'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "price_data_df = pd.read_csv('../new_values.csv')\n",
    "crypto_regulatory_pr_positive_df = crypto_regulatory_pr_df[crypto_regulatory_pr_df['positive_sentiment'] == 1]\n",
    "crypto_regulatory_pr_negative_df = crypto_regulatory_pr_df[crypto_regulatory_pr_df['negative_sentiment'] == 1]\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=price_data_df['timestamp'], y=price_data_df['Forward MDD'], name='Forward MDD'),\n",
    "            secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=price_data_df['timestamp'], y=price_data_df[f'close'], name=f'Price'),\n",
    "            secondary_y=False)\n",
    "fig.add_trace(go.Scatter(   x=crypto_regulatory_pr_positive_df['timestamp'], y=crypto_regulatory_pr_positive_df['positive_sentiment'], \n",
    "                            name='Positive Sentiment', line_shape='hvh', mode='markers',\n",
    "                            line=dict(color='teal', width=4),\n",
    "                            text=crypto_regulatory_pr_positive_df['title']),\n",
    "            secondary_y=True)\n",
    "fig.add_trace(go.Scatter(   x=crypto_regulatory_pr_negative_df['timestamp'], y=crypto_regulatory_pr_negative_df['negative_sentiment'], \n",
    "                            name='Negative Sentiment', line_shape='hvh', mode='markers',\n",
    "                            line=dict(color='red', width=4),\n",
    "                            text=crypto_regulatory_pr_positive_df['title']),\n",
    "            secondary_y=True)\n",
    "fig.update_layout(title_text=\"Forward MDD vs Sentiment for Crypto related Press Releases\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "price_data_df = pd.read_csv('../new_values.csv')\n",
    "regulatory_pr_positive_df = regulatory_pr_df[regulatory_pr_df['positive_sentiment'] == 1]\n",
    "regulatory_pr_negative_df = regulatory_pr_df[regulatory_pr_df['negative_sentiment'] == 1]\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=price_data_df['timestamp'], y=price_data_df['Forward MDD'], name='Forward MDD'),\n",
    "            secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=price_data_df['timestamp'], y=price_data_df[f'close'], name=f'Price'),\n",
    "            secondary_y=False)\n",
    "fig.add_trace(go.Scatter(   x=regulatory_pr_positive_df['timestamp'], y=regulatory_pr_positive_df['positive_sentiment'], \n",
    "                            name='Positive Sentiment', line_shape='hvh', mode='markers',\n",
    "                            line=dict(color='teal', width=4),\n",
    "                            text=regulatory_pr_positive_df['title']),\n",
    "            secondary_y=True)\n",
    "fig.add_trace(go.Scatter(   x=regulatory_pr_negative_df['timestamp'], y=regulatory_pr_negative_df['negative_sentiment'], \n",
    "                            name='Negative Sentiment', line_shape='hvh', mode='markers',\n",
    "                            line=dict(color='red', width=4),\n",
    "                            text=regulatory_pr_positive_df['title']),\n",
    "            secondary_y=True)\n",
    "fig.update_layout(title_text=\"Forward MDD vs Sentiment for all Press Releases\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analyze Change in FMDD for PR release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hours_delta in [1,2,4]:\n",
    "\n",
    "    fmdd_increase = list()\n",
    "    price_increase = list()\n",
    "    positive_pr_fmmdd_increase = 0\n",
    "    positive_pr_price_increase = 0\n",
    "    negative_pr_fmmdd_increase = 0\n",
    "    negative_pr_price_increase = 0\n",
    "    positive_pr_count = 0\n",
    "    negative_pr_count = 0\n",
    "    for i, row in crypto_regulatory_pr_df.iterrows():\n",
    "        # Get FMDD value for timestamp in price_data_df just before the news release\n",
    "        timestamp_start, price_start, fmdd_start = price_data_df[price_data_df['timestamp'] <= row['timestamp']].iloc[-1][['timestamp', 'close', 'Forward MDD']].values\n",
    "        timestamp_end = str(datetime.datetime.strptime(timestamp_start, '%Y-%m-%d %H:%M:%S') + datetime.timedelta(hours=hours_delta))\n",
    "        fmdd_mean = np.mean(price_data_df[(price_data_df['timestamp'] > timestamp_start) & (price_data_df['timestamp'] <= timestamp_end)]['Forward MDD'].values)\n",
    "        price_mean = np.mean(price_data_df[(price_data_df['timestamp'] > timestamp_start) & (price_data_df['timestamp'] <= timestamp_end)]['close'].values)\n",
    "        if fmdd_mean > fmdd_start:\n",
    "            fmdd_increase.append(1)\n",
    "        else:\n",
    "            fmdd_increase.append(0)\n",
    "        if price_mean > price_start:\n",
    "            price_increase.append(1)\n",
    "        else:\n",
    "            price_increase.append(0)\n",
    "\n",
    "        if row['positive_sentiment'] == 1:\n",
    "            positive_pr_count += 1\n",
    "            if fmdd_increase[-1] == 1:\n",
    "                positive_pr_fmmdd_increase += 1\n",
    "            if price_increase[-1] == 1:\n",
    "                positive_pr_price_increase += 1\n",
    "        if row['negative_sentiment'] == 1:\n",
    "            negative_pr_count += 1\n",
    "            if fmdd_increase[-1] == 1:\n",
    "                negative_pr_fmmdd_increase += 1\n",
    "            if price_increase[-1] == 1:\n",
    "                negative_pr_price_increase += 1\n",
    "        \n",
    "    positive_pr_fmmdd_increase = positive_pr_fmmdd_increase/positive_pr_count if positive_pr_count > 0 else 0\n",
    "    positive_pr_price_increase = positive_pr_price_increase/positive_pr_count if positive_pr_count > 0 else 0\n",
    "    negative_pr_fmmdd_increase = negative_pr_fmmdd_increase/negative_pr_count if negative_pr_count > 0 else 0\n",
    "    negative_pr_price_increase = negative_pr_price_increase/negative_pr_count if negative_pr_count > 0 else 0\n",
    "\n",
    "    # crypto_regulatory_pr_df['fmdd_increase'] = fmdd_increase\n",
    "    # crypto_regulatory_pr_df['price_increase'] = price_increase\n",
    "\n",
    "    print(f'Crypto only Press Releases(PR) Num: {positive_pr_count+negative_pr_count} within {hours_delta} hours')\n",
    "    print(f\"Positive PR FMDD decrease: {round(100*(1-positive_pr_fmmdd_increase),2)}%\")\n",
    "    print(f\"Negative PR FMDD increase: {round(100*(negative_pr_fmmdd_increase),2)}%\")\n",
    "    print(f\"Positive PR Price increase: {round(100*(positive_pr_price_increase),2)}%\")\n",
    "    print(f\"Negative PR Price decrease: {round(100*(1-negative_pr_price_increase),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "387e88cbe8050b647ec0430589b39a258f16fc0a17b5df8261967f13a5e410fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
