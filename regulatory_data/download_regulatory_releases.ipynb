{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def federal_reserve_datetime_conversion(date_string, time_string):\n",
    "    \"\"\"\n",
    "    Convert date and time string from Federal Reserve press release format to UTC datetime object\n",
    "    eg.. date_string = 'January 06, 2021'\n",
    "         time_string = '2:00 p.m. EST'\n",
    "    \"\"\"\n",
    "\n",
    "    time_string = time_string.split(' ')\n",
    "    if time_string[1] == 'p.m.' and time_string[0].split(':')[0] != '12':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 12) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[1] == 'a.m.' and time_string[0].split(':')[0] == '12':\n",
    "        time_string[0] = '00:' + time_string[0].split(':')[1]\n",
    "    if time_string[2] == 'EST':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 5) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[2] == 'EDT':\n",
    "        time_string[0] = str(int(time_string[0].split(':')[0]) + 4) + ':' + time_string[0].split(':')[1]\n",
    "    elif time_string[2] == 'UTC':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'Timezone not recognized: {time_string}')\n",
    "\n",
    "    time_string = time_string[0]\n",
    "\n",
    "    # Parse the time string into a time object\n",
    "    time_obj = datetime.datetime.strptime(time_string, '%H:%M')\n",
    "    # Get the current date in Eastern time zone\n",
    "    date_obj = datetime.datetime.strptime(date_string, '%B %d, %Y').date()\n",
    "\n",
    "    # Combine the date and time objects\n",
    "    date_obj = datetime.datetime.combine(date_obj, time_obj.time())\n",
    "\n",
    "    return str(date_obj)\n",
    "\n",
    "federal_reserve_datetime_conversion('January 06, 2021', '3:45 p.m. EDT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df_columns = ['date', 'url', 'title', 'text', 'summary', 'source', 'tags', 'type', 'language', 'country']\n",
    "\n",
    "parse_urls = [\n",
    "    {   'url': 'https://www.federalreserve.gov/newsevents/pressreleases/',\n",
    "        'source': 'Federal Reserve',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "        ]\n",
    "\n",
    "url_parsed_dict = dict()\n",
    "regulatory_news_df = pd.DataFrame()\n",
    "debugging = True\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "for parse_url in parse_urls:\n",
    "\n",
    "    if parse_url['source'] == 'Federal Reserve':\n",
    "        \n",
    "        for n in range(int((end_date - start_date).days)):\n",
    "            for category in ['monetary', 'other', 'orders', 'bcreg', 'enforcement']:\n",
    "\n",
    "                char_append = 'a'\n",
    "                while True:\n",
    "                    response = requests.get(parse_url['url'] + category + (start_date + datetime.timedelta(n)).strftime('%Y%m%d') + char_append + '.htm')\n",
    "                    # print(reponse.url)\n",
    "                    if response.status_code != 200:\n",
    "                        break\n",
    "\n",
    "                    # increment char_append\n",
    "                    char_append = chr(ord(char_append) + 1)\n",
    "\n",
    "                    if debugging:\n",
    "                        print(f\"Valid: {response.url}\")\n",
    "                    processed_row = dict() \n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    date_string = soup.find('p', class_='article__time').text.strip()\n",
    "                    time_string = soup.find('p', class_='releaseTime')\n",
    "                    if 'immediate' in time_string.text:\n",
    "                        time_string = '00:00 a.m. UTC'\n",
    "                    else:\n",
    "                        time_string = time_string.text.split('at')[-1].split('T')[0].strip() + 'T'\n",
    "\n",
    "                    time_string = time_string.replace('  ', ' ')\n",
    "\n",
    "                    processed_row['date'] = federal_reserve_datetime_conversion(date_string, time_string)\n",
    "                    if debugging:\n",
    "                        print(processed_row['date'], date_string, time_string)\n",
    "\n",
    "                    processed_row['url'] = response.url\n",
    "                    if processed_row['url'] not in url_parsed_dict:\n",
    "                        url_parsed_dict[processed_row['url']] = True\n",
    "                    else:\n",
    "                        continue\n",
    "                    processed_row['title'] = soup.find('h3', class_='title').text.strip()\n",
    "                    processed_row['text'] = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8').text.strip()\n",
    "                    processed_row['summary'] = ''\n",
    "                    processed_row['source'] = parse_url['source']\n",
    "                    processed_row['tags'] = parse_url['tags']\n",
    "                    processed_row['type'] = parse_url['type']\n",
    "                    processed_row['language'] = parse_url['language']\n",
    "                    processed_row['country'] = parse_url['country']\n",
    "\n",
    "                    if debugging:\n",
    "                        print(processed_row)\n",
    "\n",
    "                    processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "                    regulatory_news_df = pd.concat([regulatory_news_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "regulatory_news_df.to_csv('regulatory_news_fed.csv', index=False)\n",
    "regulatory_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regulatory_news.csv')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_urls = [\n",
    "    {   'url': 'https://www.federalreserve.gov/newsevents/pressreleases/',\n",
    "        'source': 'Federal Reserve',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.fdic.gov/news/press-releases/2021/pr21\",\n",
    "        'source': 'FDIC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.fdic.gov/news/press-releases/2022/pr22\",\n",
    "        'source': 'FDIC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=crypto&year=2021&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'crypto',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=&year=2021&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=crypto&year=2022&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'crypto',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "    {   'url': \"https://www.sec.gov/news/pressreleases?aId=&combine=&year=2022&month=All\",\n",
    "        'source': 'SEC',\n",
    "        'tags': 'general',\n",
    "        'type': 'press release',\n",
    "        'language': 'en',\n",
    "        'country': 'US'\n",
    "    },\n",
    "        ]\n",
    "\n",
    "url_parsed_dict = dict()\n",
    "regulatory_news_df = pd.DataFrame()\n",
    "debugging = False\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "for parse_url in parse_urls:\n",
    "\n",
    "    if parse_url['source'] == 'Federal Reserve':\n",
    "        \n",
    "        for n in range(int((end_date - start_date).days)):\n",
    "            for category in ['monetary', 'other', 'orders', 'bcreg', 'enforcement']:\n",
    "\n",
    "                char_append = 'a'\n",
    "                while True:\n",
    "                    response = requests.get(parse_url['url'] + category + (start_date + datetime.timedelta(n)).strftime('%Y%m%d') + char_append + '.htm')\n",
    "                    # print(reponse.url)\n",
    "                    if response.status_code != 200:\n",
    "                        break\n",
    "\n",
    "                    # increment char_append\n",
    "                    char_append = chr(ord(char_append) + 1)\n",
    "\n",
    "                    if debugging:\n",
    "                        print(f\"Valid: {response.url}\")\n",
    "                    processed_row = dict() \n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    date_string = soup.find('p', class_='article__time').text.strip()\n",
    "                    time_string = soup.find('p', class_='releaseTime')\n",
    "                    if 'immediate' in time_string.text:\n",
    "                        time_string = '00:00 a.m. UTC'\n",
    "                    else:\n",
    "                        time_string = time_string.text.split('at')[-1].split('T')[0].strip() + 'T'\n",
    "\n",
    "                    time_string = time_string.replace('  ', ' ')\n",
    "\n",
    "                    processed_row['date'] = federal_reserve_datetime_conversion(date_string, time_string)\n",
    "                    if debugging:\n",
    "                        print(processed_row['date'], date_string, time_string)\n",
    "\n",
    "                    processed_row['url'] = response.url\n",
    "                    if processed_row['url'] not in url_parsed_dict:\n",
    "                        url_parsed_dict[processed_row['url']] = True\n",
    "                    else:\n",
    "                        continue\n",
    "                    processed_row['title'] = soup.find('h3', class_='title').text.strip()\n",
    "                    processed_row['text'] = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8').text.strip()\n",
    "                    processed_row['summary'] = ''\n",
    "                    processed_row['source'] = parse_url['source']\n",
    "                    processed_row['tags'] = parse_url['tags']\n",
    "                    processed_row['type'] = parse_url['type']\n",
    "                    processed_row['language'] = parse_url['language']\n",
    "                    processed_row['country'] = parse_url['country']\n",
    "\n",
    "                    if debugging:\n",
    "                        print(processed_row)\n",
    "\n",
    "                    processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "                    regulatory_news_df = pd.concat([regulatory_news_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "    if parse_url['source'] == 'FDIC':\n",
    "\n",
    "        i = 1\n",
    "        while True:\n",
    "            response = requests.get(parse_url['url'] + str(i).zfill(3) + '.html')\n",
    "            if response.status_code != 200:\n",
    "                if debugging:\n",
    "                    print(f'pr21{str(i).zfill(3)}.html not found. Breaking loop.')\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "            processed_row = dict() \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            date_string = soup.find('span', class_='prdate').text.strip()\n",
    "            if len(date_string.split(',')) == 3:\n",
    "                processed_row['date'] = datetime.datetime.strptime(date_string, '%A, %B %d, %Y').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            elif len(date_string.split(',')) == 2:\n",
    "                processed_row['date'] = datetime.datetime.strptime(date_string, '%B %d, %Y').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            processed_row['url'] = response.url\n",
    "            if processed_row['url'] not in url_parsed_dict:\n",
    "                url_parsed_dict[processed_row['url']] = True\n",
    "            else:\n",
    "                continue\n",
    "            processed_row['title'] = soup.find_all('div', class_='prtitle')[0].find('h1').text.strip()\n",
    "            \n",
    "            processed_row['text'] = ''\n",
    "            for paragraph in soup.find('article', class_='order-2').find_all('p'):\n",
    "                processed_row['text'] += paragraph.text.strip() + '\\n'\n",
    "            processed_row['summary'] = ''\n",
    "            processed_row['source'] = parse_url['source']\n",
    "            processed_row['tags'] = parse_url['tags']\n",
    "            processed_row['type'] = parse_url['type']\n",
    "            processed_row['language'] = parse_url['language']\n",
    "            processed_row['country'] = parse_url['country']\n",
    "\n",
    "            if debugging:\n",
    "                print(processed_row)\n",
    "\n",
    "            processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "            regulatory_news_df = pd.concat([regulatory_news_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "    if parse_url['source'] == 'SEC':\n",
    "\n",
    "        # continue\n",
    "\n",
    "        response = requests.get(parse_url['url'])\n",
    "        # print(response.status_code)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        articles = soup.find_all('tr', class_='pr-list-page-row')\n",
    "\n",
    "        for article in reversed(articles):\n",
    "\n",
    "\n",
    "            processed_row = dict() \n",
    "            row = article.find_all('td')\n",
    "\n",
    "            # 1. Time\n",
    "            processed_row['date'] = parser.parse(row[1].find('time')['datetime']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # 2. URL\n",
    "            processed_row['url'] = \"https://www.sec.gov\" + row[2].find('a')['href']\n",
    "            if processed_row['url'] not in url_parsed_dict:\n",
    "                url_parsed_dict[processed_row['url']] = True\n",
    "            else:\n",
    "                continue\n",
    "            # 3. Title\n",
    "            processed_row['title'] = row[2].find('a').text.strip()\n",
    "            # 4. Text\n",
    "            article_soup = BeautifulSoup(requests.get(processed_row['url']).content, 'html.parser')\n",
    "            processed_row['text'] = article_soup.find('div', class_='article-body').text.strip()\n",
    "            # 5. Summary\n",
    "            processed_row['summary'] = ''\n",
    "            # 6. Source\n",
    "            processed_row['source'] = parse_url['source']\n",
    "            # 7. Tags\n",
    "            processed_row['tags'] = parse_url['tags']\n",
    "            # 8. Type\n",
    "            processed_row['type'] = parse_url['type']\n",
    "            # 9. Language\n",
    "            processed_row['language'] = parse_url['language']\n",
    "            # 10. Country\n",
    "            processed_row['country'] = parse_url['country']\n",
    "\n",
    "            if debugging:\n",
    "                print(processed_row)\n",
    "\n",
    "            processed_row_df = pd.DataFrame(processed_row, index=[0])\n",
    "            regulatory_news_df = pd.concat([regulatory_news_df, processed_row_df], ignore_index=True, axis=0)\n",
    "\n",
    "regulatory_news_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEC = 507 rows \\\n",
    "FDIC = 197 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulatory_news_df.to_csv('regulatory_news.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "387e88cbe8050b647ec0430589b39a258f16fc0a17b5df8261967f13a5e410fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
